---
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
```


<p align="left"><img width="60%" src="vignettes/imgs/ONNX_logo_main.png"/><img width="20%" src="vignettes/imgs/Rlogo.png"/></p>


## Overview

This is the R Interface to [Open Neural Network Exchange (ONNX)](https://onnx.ai/) - a standard format for models built using different frameworks (e.g. TensorFlow, MXNet, PyTorch, CNTK, etc.). It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. Models trained in one framework can be easily transferred to another framework for inference. This open source format enables the interoperability between different frameworks and streamlining the path from research to production will increase the speed of innovation in the AI community.

Deep learning with neural networks is accomplished through computation over dataflow graphs. Some frameworks (such as CNTK, Caffe2, Theano, and TensorFlow) make use of static graphs, while others (such as PyTorch and Chainer) use dynamic graphs. However, they all provide interfaces that make it simple for developers to construct computation graphs and runtimes that process the graphs in an optimized way. The graph serves as an Intermediate Representation (IR) that captures the specific intent of the developer's source code, and is conducive for optimization and translation to run on specific devices (CPU, GPU, FPGA, etc.).

## Why ONNX?

Today, each framework has its own proprietary representation of the graph, though they all provide similar capabilities â€“ meaning each framework is a siloed stack of API, graph, and runtime. Furthermore, frameworks are typically optimized for some characteristic, such as fast training, supporting complicated network architectures, inference on mobile devices, etc. It's up to the developer to select a framework that is optimized for one of these characteristics. Additionally, these optimizations may be better suited for particular stages of development. This leads to significant delays between research and production due to the necessity of conversion.

With the goal of democratizing AI, we envision empowering developers to select the framework that works best for their project, at any stage of development or deployment. The ONNX format is a common IR to help establish this powerful ecosystem. By providing a common representation of the computation graph, ONNX helps developers choose the right framework for their task, allows authors to focus on innovative enhancements, and enables hardware vendors to streamline optimizations for their platforms.

For more details on the technical design of ONNX spec, please visit [here](https://github.com/onnx/onnx#learn-about-onnx-spec).

## Installation

* First, follow instructions [here](https://github.com/onnx/onnx#installation) to install the onnx Python package.
* Then install the development version of the R package on Github via the following:

```
if (!require(devtools)) install.packages("devtools")
devtools::install_github("onnx/onnx-r")
```

## Tutorials

* [Creating ONNX Protobuf](articles/protobufs.html)
* [Load and Run an ONNX Model](articles/onnx_model_zoo.html)

## Discuss

We encourage you to open [Issues](https://github.com/onnx/onnx-r/issues), or use Gitter for more real-time discussion:
[![Join the chat at https://gitter.im/onnx/Lobby](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/onnx/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

## Follow Us

Stay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter](https://twitter.com/onnxai)].
